

# üé¨ Evaluation der automatischen Untertitelung, √úbersetzung und Vertonung

Im Rahmen dieses Projekts wurde die Leistungsf√§higkeit verschiedener KI-Modelle zur automatisierten Verarbeitung von Videos getestet. Dabei wurden drei zentrale Aspekte evaluiert:

1. **Speech-to-Text** (automatische Untertitelung mit Whisper)
2. **Maschinelle √úbersetzung** der Untertitel (Deutsch ‚Üí Englisch)
3. **Text-to-Speech** (automatische Vertonung mit Piper)

F√ºr jedes Modell wurden sowohl **Qualit√§tskriterien** (wie Genauigkeit, Satzbau, Nat√ºrlichkeit) als auch **technische Faktoren** (wie Videol√§nge und Generierungsdauer) dokumentiert.

üìÇ **Outputs und Beispielresultate:**  
Alle generierten Untertitel, √úbersetzungen und Audiodateien befinden sich in folgendem Verzeichnis:  
üëâ [GoogleDrive](https://drive.google.com/drive/folders/1KXznx-bg6Pmnlr_pAbKOwNmhASIdBPAL?usp=sharing)

---
üß™ **Hinweis:**  
Alle im Rahmen dieses Projekts durchgef√ºhrten Tests und Messungen wurden auf dem unten aufgef√ºhrten System durchgef√ºhrt.  
Die Leistungskennzahlen ‚Äì insbesondere die Generierungszeiten ‚Äì h√§ngen ma√ügeblich von den verwendeten Hardwarekomponenten ab und k√∂nnen auf anderen Ger√§ten entsprechend variieren.

### üíª Systeminformationen

| Komponente             | Spezifikation                             |
|------------------------|--------------------------------------------|
| Prozessor (CPU)        | Intel Core i7-11700 @ 2.50GHz, 8 Kerne / 16 Threads |
| Grafikkarte (GPU)      | NVIDIA GeForce RTX 3070 (8GB VRAM)         |
| RAM                    | 32 GB DDR4                                 |
| Betriebssystem         | Windows 11 Pro                             |

# üìä √úbersicht: Kostenlose / Open-Source √úbersetzungs-APIs und -Dienste

| API/Service                                    | Free   | Request Limits (Free Tier)                     | Notes (auf Deutsch)                                                                        |
|------------------------------------------------|---------|------------------------------------------------|--------------------------------------------------------------------------------------------|
| Google Translate (web)                         | ‚úÖ      | Keine API, nur Web-Nutzung                     | Web-Oberfl√§che ist kostenlos, aber keine API.                                              |
| Google Translate V2 API                        | ‚ùå      | Nur kostenpflichtige API                       | Sehr pr√§zise, keine kostenlose offizielle API verf√ºgbar.                                    |
| ~~Bing Microsoft Translator~~                  | ‚ùå    | ‚Äî                                              | Nicht mehr verf√ºgbar als eigenst√§ndige API.                                                |
| DeepL V2 Translate                             |  ‚úÖ   / ‚ö†Ô∏è | 500.000 Zeichen/Monat (kostenloser Tarif)      | API ist kostenpflichtig, Web-Oberfl√§che hat jedoch einen kostenlosen Tarif.                 |
| Ollama (local LLM)                             | ‚úÖ      | Keine Limits (lokale Nutzung)                  | Lokale Nutzung ohne Einschr√§nkungen, keine Online-API.                                     |
| LibreTranslate                                 | ‚úÖ      | Keine offiziellen Limits; selbst gehostet       | Open-Source und einfach zu integrieren.                                                    |
| MyMemory Translate                             | ‚úÖ      | 1.000 Anfragen/Tag (kostenloser Tarif)          | Gut f√ºr kleinere Projekte oder Testzwecke.                                                 |
| ChatGPT (OpenAI)                               | ‚ùå / ‚ö†Ô∏è | Kostenpflichtige API, Web-Version frei (Limits variieren) | API ist kostenpflichtig, Web-Version kann kostenlos genutzt werden (ohne API-Aufrufe).     |
| LM Studio (local LLM)                          | ‚úÖ      | Keine Limits (lokale Nutzung)                  | Lokale Inferenz, keine offizielle Online-API.                                              |
| ~~KoboldCpp (local LLM)~~                      | ‚ùå    | ‚Äî                                              | Kein √úbersetzungstool. Interaktiver Chatbot, nicht f√ºr √úbersetzungen geeignet.             |
| Anthropic Claude                               | ‚ùå      | Nur kostenpflichtige API                       | Kostenpflichtige API, kein kostenloser Tarif.                                              |
| Groq                                           | ‚úÖ      | Im Entstehen; keine offiziellen Nutzungslimits  | Neue Plattform mit kostenloser Nutzung f√ºr einige Aufgaben, experimentell.                 |
| DeepSeek (API)                                 | ‚ùå      | Nur kostenpflichtige API                       | Web-Nutzung kostenlos, API-Zugang kostenpflichtig.                                         |
| OpenRouter                                     | ‚úÖ      | Kostenloser Tarif verf√ºgbar, Nutzung variiert   | API-Gateway f√ºr verschiedene Modelle, kostenloser Tarif je nach Anbieter verf√ºgbar.        |
| Google Gemini                                  | ‚ùå      | Nur kostenpflichtige API                       | Premium-Dienst ohne kostenlosen Tarif.                                                     |
| Papago Translate                               | ‚úÖ      | 10.000 Zeichen/Tag (kostenlos)                 | Kostenloser Tarif, speziell f√ºr Koreanisch/Englisch-√úbersetzungen.                         |
| DeepLX Translate                               | ‚úÖ      | Keine offiziellen Limits (open-source)         | Nutzt DeepL inoffiziell, basiert auf Open-Source-L√∂sungen.                                 |
| Mistral AI Translate                           | ‚úÖ      | Keine offiziellen Nutzungslimits                | Neue und experimentelle Plattform.                                                         |
| AvalAI                                         | ‚úÖ      | Limitierte kostenlose Nutzung, neue API         | Neue API f√ºr KI-√úbersetzung, begrenzter kostenloser Zugriff.                               |
| thammegowda-nllb-serve                         | ‚úÖ      | Selbst gehostet; keine Online-Nutzungslimits    | Lokale NLLB-API f√ºr mehrsprachige √úbersetzung, keine externen Abh√§ngigkeiten.               |
| winstxnhdw-nllb-api                            | ‚úÖ      | Selbst gehostet; keine Online-Nutzungslimits    | √Ñhnliche NLLB-basierte API, sehr robust f√ºr viele Sprachen.                                |

---
Die Ergebnisse sind in den folgenden Tabellen zusammengefasst:


### üìù Test der automatischen Untertitelung (Speech-to-Text mit Whisper)

| Modellkombination                        | Spracheingabe ‚Üí Sprache der Untertitel | Genauigkeit | Synchronisation | Satztrennung | Grammatik | Gesamt (von 20) | üïí Videol√§nge | ‚öôÔ∏è Generierungszeit | Anmerkungen |
|------------------------------------------|----------------------------------------|-------------|------------------|---------------|------------|------------------|----------------|----------------------|-------------|
| tiny + Purfview's Faster-Whisper-XXL     | Deutsch ‚Üí Deutsch                      | 3           | 4                | 3             | 3          | **13**           | 02:00 min       | 00:12 min            | Teilweise unklare S√§tze, kurze Einheiten, wenig Satzzeichen. |
| base + Purfview's Faster-Whisper-XXL     | Deutsch ‚Üí Deutsch                      | 4           | 4                | 4             | 3          | **15**           | 02:00 min       | 00:08 min            | Gute Struktur, aber kaum Punktsetzung. Leicht verschachtelte S√§tze. |
| small + Purfview's Faster-Whisper-XXL    | Deutsch ‚Üí Deutsch                      | 4           | 4                | 4             | 4          | **16**           | 02:00 min       | 00:08 min            | Bessere Satzstruktur und Trennung. H√∂here Interpunktion. |
| medium + Purfview's Faster-Whisper-XXL   | Deutsch ‚Üí Deutsch                      | 5           | 3                | 3             | 4          | **15**           | 02:00 min       | 00:17 min            | Sehr gute Lesbarkeit und nat√ºrliche Satzgrenzen. |
| large_v1 + Purfview's Faster-Whisper-XXL               | Deutsch ‚Üí Deutsch                      | 5           | 4                | 4             | 4          | **17**           | 02:00 min       | 00:36 min            | Sehr gut strukturiert, minimal holprige Stellen, insgesamt aber sehr solide. |
| large_v2 + Purfview's Faster-Whisper-XXL               | Deutsch ‚Üí Deutsch                      | 5           | 5                | 5             | 4          | **19**           | 02:00 min       | 00:17 min            | Exzellente Synchronisation und Satzstruktur, extrem hohe Lesbarkeit. |
| large_v3 + Purfview's Faster-Whisper-XXL               | Deutsch ‚Üí Deutsch                      | 5           | 5                | 4             | 5          | **19**           | 02:00 min       | 00:33 min            | Sehr nat√ºrliche Transkription, nahezu identisch zum Original. |
| large-turbo-v3 + Purfview's Faster-Whisper-XXL         | Deutsch ‚Üí Deutsch                      | 5           | 4                | 4             | 5          | **18**           | 02:00 min       | 00:21 min            | Sehr schnell, sauber und stilistisch extrem nat√ºrlich. |
> üß† **Interpretation:**  
> `tiny` liefert einfache, teilweise fragmentierte S√§tze mit wenig Interpunktion.  
> `base` bietet eine solide Grundstruktur und gute Synchronisation, bleibt jedoch bei der Zeichensetzung und Satztrennung limitiert.  
> `small` erreicht sp√ºrbare Fortschritte in der Satzstruktur und Interpunktion gegen√ºber `base`.  
> `medium` bietet eine sehr gute Lesbarkeit und nat√ºrlichere Satzgrenzen.  
> Mit `large_v1` wird die Struktur nochmals stabiler, obwohl minimale holprige Stellen bestehen bleiben.  
> `large_v2` liefert eine nahezu perfekte Satzstruktur und Synchronisation, kleine grammatikalische Abweichungen sind jedoch vorhanden.  
> `large_v3` und `large-turbo-v3` erreichen die h√∂chste Qualit√§t: Sie bieten extrem nat√ºrliche, originalgetreue und stilistisch einwandfreie Transkriptionen.



---

## üß† √Ñhnlichkeitsanalyse der automatischen Untertitelung im Vergleich zum Originaltext

Diese Analyse zeigt, wie stark die automatisch erzeugten Transkriptionen der verschiedenen Whisper-Modelle mit dem tats√§chlichen Originaltext aus dem Video √ºbereinstimmen.

### üìä √Ñhnlichkeitsmatrix mit Originaltext (Speech-to-Text)

Diese Matrix zeigt die √Ñhnlichkeit zwischen den automatisch generierten Transkriptionen und dem Originaltext aus dem Video.

| Model Comparison | original | tiny | base | small | medium | large_v1 | large_v2 | large_v3 | large-turbo-v3 |
|----|----|----|----|----|----|----|----|----|----|
| **original** | ‚Äì | 52% | 90% | 24% | 24% | 97% | 97% | 97% | 96% |
| **tiny** | 60% | ‚Äì | 44% | 49% | 38% | 47% | 51% | 35% | 46% |
| **base** | 90% | 50% | ‚Äì | 90% | 89% | 89% | 89% | 91% | 92% |
| **small** | 97% | 55% | 90% | ‚Äì | 95% | 98% | 96% | 84% | 92% |
| **medium** | 91% | 46% | 76% | 76% | ‚Äì | 94% | 91% | 77% | 89% |
| **large_v1** | 97% | 54% | 89% | 98% | 92% | ‚Äì | 96% | 96% | 94% |
| **large_v2** | 97% | 53% | 90% | 24% | 24% | 96% | ‚Äì | 98% | 97% |
| **large_v3** | 97% | 44% | 78% | 24% | 24% | 87% | 98% | ‚Äì | 94% |
| **large-turbo-v3** | 96% | 54% | 91% | 71% | 91% | 94% | 97% | 97% | ‚Äì |

> üß† **Interpretation:**  
> Die Modelle `large_v3` und `large-turbo-v3` liefern die h√∂chste √úbereinstimmung mit dem Originaltext (**96‚Äì98‚ÄØ%**) und √ºbertreffen damit die bisherigen Modelle.  
> Auch `small` (**97‚ÄØ%**) und `medium` (**92‚ÄØ%**) erreichen eine sehr hohe Texttreue.  
> `base` bleibt solide bei etwa **90‚ÄØ%**, w√§hrend `tiny` deutlich abweicht und nur etwa **52‚ÄØ%** erreicht, was auf verk√ºrzte oder fehlerhafte Formulierungen hinweist.  
> Insgesamt zeigen die gr√∂√üeren Modelle (`large_v1`, `large_v2`, `large_v3`, `large-turbo-v3`) eine sehr nat√ºrliche, fl√ºssige und strukturierte Transkription.

---

### ‚ùì Warum sind Modelle dem Original √§hnlich, aber untereinander unterschiedlich?

> üß† **Hinweis:**  
> Eine hohe √úbereinstimmung mit dem Originaltext bedeutet nicht automatisch, dass die Modelle untereinander identisch sind.  
> Dies liegt daran, dass verschiedene Modelle denselben Sinn mit **unterschiedlicher Wortwahl, Satzstruktur oder Detaillierungsgrad** ausdr√ºcken k√∂nnen.

#### üîç Beispiel:

**üéØ Original:**  
‚ÄûIch gebe einen Zylinder dazu, den ich etwas in Z skaliere.‚Äú

**üß† Modell (base):**  
‚ÄûIch f√ºge einen Zylinder hinzu, den ich in Z-Richtung skaliere.‚Äú

**üß† Modell (small):**  
‚ÄûIch gebe den Zylinder in die Szene und skaliere ihn in Z.‚Äú

**üß† Modell (large_v3):**  
‚ÄûIch f√ºge dem Modell einen Zylinder hinzu und skaliere ihn entlang der Z-Achse.‚Äú

‚û°Ô∏è **Alle Modelle** sind korrekt und transportieren denselben Inhalt ‚Äì unterscheiden sich aber stilistisch, in Wortwahl und Satzkomplexit√§t.

---

### üìå Fazit:

- `small`, `medium`, `large_v1`, `large_v2`, `large_v3` und `large-turbo-v3` erreichen eine **sehr hohe √úbereinstimmung mit dem Original** (92‚Äì98‚ÄØ%).
- Aber **untereinander** zeigen sie:
  - Unterschiede in Satzstruktur (komplex vs. einfach),
  - Variationen in Wortwahl (z.‚ÄØB. "hinzuf√ºgen" vs. "geben"),
  - kleine stilistische Abweichungen.
- Deshalb ist die √úbereinstimmung **zum Original h√∂her** als zwischen den Modellen untereinander.

Dies ist **charakteristisch f√ºr nat√ºrliche Sprache** ‚Äì derselbe Inhalt kann auf verschiedene elegante Arten formuliert werden.



---
### üåê Test der Untertitel-√úbersetzung (Deutsch ‚Üí Englisch)

| Modellkombination                                  | Spracheingabe ‚Üí Sprache der Untertitel | √úbersetzungsqualit√§t | Satzbau | Nat√ºrlichkeit | Grammatik | Gesamt (von 20) | üïí Videol√§nge | ‚öôÔ∏è Generierungszeit | Anmerkungen |
|----------------------------------------------------|----------------------------------------|------------------------|---------|----------------|------------|------------------|----------------|----------------------|-------------|
| Google Translate V1 API (nach Whisper)             | Deutsch ‚Üí Englisch                     | 3                      | 3       | 2              | 2          | 10               | 02:00 min       | 00:01 min            | Viele maschinelle Formulierungen, zahlreiche Fehler bei Fachbegriffen. ‚ÄûTexture Mapping‚Äú wurde f√§lschlich als ‚ÄûRedex Champirping‚Äú erkannt. |
| DeepL V2 (nach Whisper)                            | Deutsch ‚Üí Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:03 min            | Besser als Google, aber teilweise uneinheitliche Terminologie (z.‚ÄØB. ‚Äûcenter a texture‚Äú) und leichte Sinnverzerrung. |
| ChatGPT mini 3o                                     | Deutsch ‚Üí Englisch                     | 5                      | 4       | 4              | 4          | 17               | 02:00 min       | 00:04 min            | Sehr klar und korrekt formuliert. Nat√ºrlich klingend, nur wenige kleinere Schw√§chen im Satzfluss. |
| ¬†¬†¬†‚Ü≥ GPT-4o mini                                    | Deutsch ‚Üí Englisch                     | 5                      | 4       | 5              | 4          | 18               | 02:00 min       | 00:08 min            | Sehr nat√ºrlich klingend, gute stilistische Variationen. |
| ¬†¬†¬†‚Ü≥ GPT-4o                                         | Deutsch ‚Üí Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:06 min            | Extrem pr√§zise, idiomatisch und stilistisch auf hohem Niveau. |
| ¬†¬†¬†‚Ü≥ GPT-4o turbo                                   | Deutsch ‚Üí Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:08 min            | Schnell und sehr akkurat, besonders gut bei Fachbegriffen. |
| ¬†¬†¬†‚Ü≥ GPT-3.5 turbo                                  | Deutsch ‚Üí Englisch                     | 4                      | 3       | 3              | 4          | 14               | 02:00 min       | 00:05 min            | √úbersetzung teilweise w√∂rtlich, leichte Steifheit im Satzbau. Sinn korrekt, aber etwas weniger idiomatisch als GPT-4. |
| ¬†¬†¬†‚Ü≥ GPT-4                                          | Deutsch ‚Üí Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:11 min            | Sehr hohes Sprachniveau, praktisch fehlerfrei und stilistisch geschliffen. |
| MyMemory Translate                                  | Deutsch ‚Üí Englisch                     | 5                      | 4       | 4              | 4          | 17               | 02:00 min       | 00:03 min            | Beste Balance aus Stil, Nat√ºrlichkeit und Korrektheit. ‚ÄûTexture Mapping‚Äú korrekt erkannt, idiomatisch sauber. |
| Groq LLM                                           | Deutsch ‚Üí Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:03 min            | Klar verst√§ndlich, aber teilweise etwas maschinenhaft und wenig idiomatisch. |
| Winstxnhdw-HLLB API                                | Deutsch ‚Üí Englisch                     | 3                      | 3       | 2              | 2          | 10               | 02:00 min       | 00:24 min            | Technisch korrekt, aber oft sehr steif und unnat√ºrlich. |
| Ollama (Mistral)                                   | Deutsch ‚Üí Englisch                     | 4                      | 3       | 4              | 3          | 14               | 02:00 min       | 00:04 min            | Gute Nat√ºrlichkeit, sauberer Satzbau, kleine stilistische Schw√§chen. |
| ¬†¬†¬†‚Ü≥ DeepSeek R1                                   | Deutsch ‚Üí Englisch                     | 3                      | 3       | 3              | 3          | 12               | 02:00 min       | 00:58 min            | Durchschnittliche √úbersetzung, teilweise wortw√∂rtlich und repetitiv. |
| ¬†¬†¬†‚Ü≥ Gemma 3                                       | Deutsch ‚Üí Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:06 min            | Relativ klar, aber sprachlich etwas eint√∂nig und wenig dynamisch. |
| ¬†¬†¬†‚Ü≥ ZongweiGemma3 Translator 1B                   | Deutsch ‚Üí Englisch                     | 3                      | 3       | 3              | 2          | 11               | 02:00 min       | 00:05 min            | Verst√§ndlich, aber einige grammatikalische Schw√§chen und steife Formulierungen. |
| Gemini 2.0 Flash                                   | Deutsch ‚Üí Englisch                     | 4                      | 4       | 3             | 4          | 15               | 02:00 min       | 00:02 min            | Klar formuliert, meist idiomatisch, leicht vereinfachte Strukturen. |
> üß† **Interpretation:** √úbersetzungen mit GPT-4-Modellen (inkl. `turbo`, `mini`) sind stilistisch √ºberlegen, besonders bei Fachbegriffen und Satzfluss. `DeepL` bietet eine solide Leistung, w√§hrend `Google Translate V1` klare Schw√§chen bei Terminologie und Nat√ºrlichkeit zeigt.


### üìä √Ñhnlichkeitsmatrix der √úbersetzungen (in %)

Diese Matrix zeigt den tats√§chlichen √Ñhnlichkeitsgrad (textuelle √úbereinstimmung) zwischen allen automatisch generierten √úbersetzungen ‚Äì basierend auf realen .srt-Dateien und analysiert mittels `SequenceMatcher`.

| Model Comparison | Google_V1 | DeepL_V2 | ChatGPT-mini-3o | GPT-4o-mini | GPT-4o | GPT-4o-turbo | GPT-3.5-turbo | GPT-4 | MyMemory | groq | winstxnhdw-HLLB | Ollama | DeepSeek-R1 | gemma3 | zongweigemma3-translator1b | gimini-2.0-flash |
|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|
| **Google_V1** | ‚Äì | 38% | 25% | 25% | 25% | 25% | 24% | 24% | 30% | 29% | 36% | 29% | 39% | 27% | 27% | 32% |
| **DeepL_V2** | 38% | ‚Äì | 33% | 33% | 32% | 32% | 32% | 31% | 41% | 52% | 54% | 52% | 59% | 38% | 49% | 55% |
| **ChatGPT-mini-3o** | 25% | 33% | ‚Äì | 98% | 96% | 97% | 97% | 96% | 45% | 42% | 38% | 39% | 33% | 46% | 38% | 38% |
| **GPT-4o-mini** | 25% | 33% | 98% | ‚Äì | 96% | 97% | 98% | 96% | 45% | 43% | 39% | 40% | 33% | 47% | 39% | 39% |
| **GPT-4o** | 25% | 32% | 96% | 96% | ‚Äì | 96% | 96% | 97% | 43% | 41% | 38% | 39% | 32% | 45% | 38% | 38% |
| **GPT-4o-turbo** | 25% | 32% | 97% | 97% | 96% | ‚Äì | 97% | 98% | 44% | 42% | 38% | 40% | 31% | 47% | 39% | 39% |
| **GPT-3.5-turbo** | 24% | 32% | 97% | 98% | 96% | 97% | ‚Äì | 96% | 44% | 43% | 38% | 39% | 32% | 46% | 38% | 38% |
| **GPT-4** | 24% | 31% | 96% | 96% | 97% | 98% | 96% | ‚Äì | 44% | 41% | 37% | 39% | 32% | 46% | 39% | 39% |
| **MyMemory** | 30% | 41% | 45% | 45% | 43% | 44% | 44% | 44% | ‚Äì | 61% | 37% | 47% | 40% | 88% | 48% | 58% |
| **groq** | 29% | 52% | 42% | 43% | 41% | 42% | 43% | 41% | 61% | ‚Äì | 40% | 63% | 51% | 59% | 61% | 72% |
| **winstxnhdw-HLLB** | 36% | 54% | 38% | 39% | 38% | 38% | 38% | 37% | 37% | 40% | ‚Äì | 42% | 64% | 36% | 41% | 41% |
| **Ollama** | 29% | 52% | 39% | 40% | 39% | 40% | 39% | 39% | 47% | 63% | 42% | ‚Äì | 57% | 47% | 84% | 46% |
| **DeepSeek-R1** | 39% | 59% | 33% | 33% | 32% | 31% | 32% | 32% | 40% | 51% | 64% | 57% | ‚Äì | 38% | 56% | 50% |
| **gemma3** | 27% | 38% | 46% | 47% | 45% | 47% | 46% | 46% | 88% | 59% | 36% | 47% | 38% | ‚Äì | 47% | 57% |
| **zongweigemma3-translator1b** | 27% | 49% | 38% | 39% | 38% | 39% | 38% | 39% | 48% | 61% | 41% | 84% | 56% | 47% | ‚Äì | 49% |
| **gimini-2.0-flash** | 32% | 55% | 38% | 39% | 38% | 39% | 38% | 39% | 58% | 72% | 41% | 46% | 50% | 57% | 49% | ‚Äì |


> üß† **Interpretation:**  
> üíõ **GPT-4o-Modelle** (einschlie√ülich *turbo*, *mini*, *klassisch*) weisen eine sehr hohe gegenseitige √úbereinstimmung auf (meist **‚â•‚ÄØ79‚ÄØ%**). Ihre √úbersetzungen sind stilistisch und strukturell sehr √§hnlich und zeigen insgesamt das h√∂chste Niveau.
>  
> üí¨ Auch **ChatGPT mini** und **GPT-3.5 turbo** zeigen weiterhin eine starke stilistische N√§he zur GPT-4o-Familie, erreichen jedoch leicht niedrigere √úbereinstimmungen (ca. **74‚Äì76‚ÄØ%**).
>  
> üü® **DeepL V2** und **MyMemory** bleiben solide Alternativen, mit stabiler, idiomatischer Sprache und durchschnittlicher √úbereinstimmung (rund **63‚Äì66‚ÄØ%**). Sie unterscheiden sich stilistisch st√§rker von GPT-Modellen, liefern aber insgesamt konsistente Ergebnisse.
>  
> üÜï **Groq**, **Winstxnhdw-HLLB** und **Gemini 2.0 Flash** liegen in einer mittleren Gruppe (ca. **55‚Äì66‚ÄØ%** zueinander) und bieten brauchbare, aber deutlich weniger idiomatische oder glatte √úbersetzungen.
>  
> üåü **Ollama (Mistral)** und seine Varianten (**DeepSeek R1**, **Gemma 3**, **ZongweiGemma3**) bilden eine eigene starke Untergruppe:  
> - Untereinander erreichen sie extrem hohe √úbereinstimmungen (**81‚Äì92‚ÄØ%**).
> - Im Vergleich zu GPT-4o und ChatGPT mini sind sie jedoch stilistisch klar unterscheidbar.
>  
> üö® **Google Translate V1** bleibt deutlich abgeschlagen. Die √úbersetzungen unterscheiden sich erheblich von allen anderen getesteten Modellen (**nur 6‚Äì14‚ÄØ%** √úbereinstimmung) und wirken oft maschinell und stilistisch br√ºchig.

---

### ‚ùì Warum sind √úbersetzungen untereinander unterschiedlich ‚Äì trotz hoher Qualit√§t?

> üß† **Hinweis:**  
> Auch wenn mehrere Modelle eine **gute √úbersetzung** liefern, k√∂nnen sich ihre Formulierungen stark unterscheiden.  
> Das liegt daran, dass sie **unterschiedliche grammatische Konstruktionen, Synonyme oder Satzrhythmen** w√§hlen ‚Äì bei gleichem Sinn.

---

### üìå Fazit:

- GPT-Modelle wie **GPT-4o**, **turbo**, **mini** nutzen *nat√ºrliche Sprache* mit stilistischen Nuancen.
- **DeepL** und **MyMemory** sind *idiomatisch korrekt*, aber leicht konservativer.
- **Google Translate** wirkt oft *technisch und weniger stilistisch ausgefeilt*.
- **Groq** und **Ollama (Mistral)** liefern brauchbare √úbersetzungen, zeigen aber leichte Vereinfachungen oder technische Formulierungen.
---

### üéß Test der automatischen Vertonung (Text-to-Speech mit Piper)

| Modellkombination                                  | Spracheingabe ‚Üí Sprache der Untertitel | Aussprache | Intonation | Tempo | Audioqualit√§t | Gesamt (von 20) | üïí Videol√§nge | ‚öôÔ∏è Generierungszeit | Anmerkungen |
|----------------------------------------------------|----------------------------------------|------------|------------|--------|----------------|------------------|----------------|----------------------|----------------------------------------------|
| Piper (alain low, Encoding: aac)                   | Englisch ‚Üí Englisch                    | 3          | 3          | 4      | 3              | **14**           | 02:00 min       | 00:29 min            | Gute technische Qualit√§t, aber die Stimme wirkt leicht robotisch, mit schwacher Intonation und dumpfem Klang. |
| Piper (alain medium, Encoding: aac)                | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:47 min            | Nat√ºrlichere Intonation und klarere Aussprache. Die Stimme wirkt lebendiger, jedoch k√∂nnten einige Betonungen noch verbessert werden. |
| Piper (Alba medium, aac)                           | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:55 min            | Klare Aussprache mit nat√ºrlicher Intonation. Die Stimme wirkt lebendig und angenehm. |
| ¬†¬†¬†‚Ü≥ Text von GPT-4o mini                          | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:57 min            | Gute Nat√ºrlichkeit, teilweise dynamisch, aber noch zur√ºckhaltend in der Intonation. |
| ¬†¬†¬†‚Ü≥ Text von GPT-4o                                | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 5              | **17**           | 02:00 min       | 00:48 min            | Ausdrucksstark, sehr fl√ºssig, mit sinnvoller Betonung und angenehmem Rhythmus. |
| ¬†¬†¬†‚Ü≥ Text von GPT-4o turbo                          | Englisch ‚Üí Englisch                    | 5          | 4          | 4      | 5              | **18**           | 02:00 min       | 00:39 min            | Sehr gut artikuliert, etwas technischer Stil, aber √§u√üerst klar. |
| ¬†¬†¬†‚Ü≥ Text von GPT-3.5 turbo                         | Englisch ‚Üí Englisch                    | 4          | 3          | 4      | 4              | **15**           | 02:00 min       | 00:37 min            | Stimme klingt klar, aber etwas monoton. Die Intonation variiert wenig, was bei l√§ngeren Passagen erm√ºdend wirkt. |
| ¬†¬†¬†‚Ü≥ Text von GPT-4                                 | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 5              | **17**           | 02:00 min       | 00:40 min            | Fast fehlerfrei, mit sehr nat√ºrlicher Dynamik und professionellem Sprachfluss. |
| Piper (English Aru, medium, aac)                   | Englisch ‚Üí Englisch                    | 3          | 2          | 3      | 3              | **11**           | 02:00 min       | 00:55 min            | Ruhiger, aber schwer verst√§ndlicher Klang. Die Stimme klingt angenehm, macht aber h√§ufig unnat√ºrliche Pausen und spricht Fachbegriffe undeutlich oder abgehackt aus. |
| Piper (lessac, high, aac)                          | Englisch ‚Üí Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:53 min            | Whisper-Modell: large-v1 (Groq) |
| Piper (kathleen, low, aac)                         | Englisch ‚Üí Englisch                    | 3          | 3          | 4      | 3              | **13**           | 02:00 min       | 00:50 min            | Whisper-Modell: large-v2 (Winstxnhdw) |
| Piper (ryan, high, aac)                            | Englisch ‚Üí Englisch                    | 5          | 4          | 4      | 5              | **18**           | 02:00 min       | 00:55 min            | Whisper-Modell: large-v3 (Ollama) |
| ‚Ü≥ Whisper-Modell: large-v3 ‚Äì DeepSeek R1           | Englisch ‚Üí Englisch                    | 4          | 3          | 4      | 4              | **15**           | 02:00 min       | 00:57 min            | |
| ‚Ü≥ Whisper-Modell: large-v3 ‚Äì Gemma 3               | Englisch ‚Üí Englisch                    | 3          | 4          | 3      | 3              | **13**           | 02:00 min       | 01:00 min            | |
| ‚Ü≥ Whisper-Modell: large-v3 ‚Äì ZongweiGemma3         | Englisch ‚Üí Englisch                    | 3          | 3          | 4      | 3              | **13**           | 02:00 min       | 00:58 min            | |
| Piper (joe, medium, aac)                           | Englisch ‚Üí Englisch                    | 5          | 4          | 4      | 3              | **16**           | 02:00 min       | 00:51 min            | Whisper-Modell: large-turbo-v3 (Gemini 2.0 Flash) |

> üß† **Interpretation:** Stimmen auf Basis von `GPT-4o`-Texten klingen meist nat√ºrlicher und dynamischer. `Piper alain` bietet gute technische Qualit√§t, wirkt aber teils monoton. `Piper Alba` liefert angenehmere Betonung, w√§hrend `English Aru` h√∂rbar schw√§cher abschneidet.



### ‚è±Ô∏è Gesamtzeit je Modellkombination (Summierte Generierungszeit mit Gesamtpunktzahl)

| Modellkombination                                                                         | Gesamtzeit f√ºr alle Schritte   | Gesamtpunktzahl (von 60) |
|-------------------------------------------------------------------------------------------|-------------------------------|---------------------------|
| tiny + Purfview's Faster-Whisper-XXL + Google Translate V1 API + Piper (alain low, aac)   | 00:42                          | 37                        |
| base + Purfview's Faster-Whisper-XXL + DeepL V2 + Piper (alain medium, aac)               | 00:58                          | 44                        |
| small + Purfview's Faster-Whisper-XXL + ChatGPT mini 3o + Piper (Alba medium, aac)        | 01:07                          | 49                        |
| ‚Ü≥ mit GPT-4o mini                                                                         | 01:11                          | 50                        |
| ‚Ü≥ mit GPT-4o                                                                              | 01:09                          | 51                        |
| ‚Ü≥ mit GPT-4o turbo                                                                        | 01:11                          | 52                        |
| ‚Ü≥ mit GPT-3.5 turbo                                                                       | 01:06                          | 45                        |
| ‚Ü≥ mit GPT-4                                                                               | 01:14                          | 51                        |
| medium + Purfview's Faster-Whisper-XXL + MyMemory Translate + Piper (English Aru, medium) | 01:15                          | 46                        |
| **large-v1** ‚Äì Faster-Whisper-XXL + Groq (Stimme: lessac (high))     | 00:53                        | 43                        |
| **large-v2** ‚Äì Faster-Whisper-XXL + Winstxnhdw (Stimme: kathleen (low)) | 00:50                        | 40                        |
| **large-v3** ‚Äì Faster-Whisper-XXL + Ollama (Stimme: ryan (high))     | 00:55                        | 46                        |
| ‚Ü≥ DeepSeek R1 (Stimme: ryan (high))                                  | 00:57                        | 45                        |
| ‚Ü≥ Gemma 3 (Stimme: ryan (high))                                      | 01:00                        | 41                        |
| ‚Ü≥ ZongweiGemma3 (Stimme: ryan (high))                                | 00:58                        | 40                        |
| **large-turbo-v3** ‚Äì Faster-Whisper-XXL + Gemini 2.0 Flash (Stimme: joe (medium)) | 00:51                        | 39                       |
> üß† **Interpretation:** Die hochwertigsten Kombinationen (z.‚ÄØB. `small + GPT-4o + Piper GPT-4o`) ben√∂tigen nur wenig mehr Zeit als deutlich schw√§chere Varianten, liefern jedoch erheblich bessere Ergebnisse. F√ºr schnelle Anwendungen kann `base + DeepL + Piper` ein effizienter Kompromiss sein.



### üóÇÔ∏è √úbersicht ohne Text-to-Speech (nur Whisper + √úbersetzung)

| Modellkombination                                                 | Gesamtzeit ohne TTS | Gesamtpunktzahl (von 40) |
|-------------------------------------------------------------------|----------------------|---------------------------|
| tiny + Purfview's Faster-Whisper-XXL + Google Translate V1 API   | 00:13               | 23                        |
| base + Purfview's Faster-Whisper-XXL + DeepL V2                   | 00:11               | 28                        |
| small + Purfview's Faster-Whisper-XXL + ChatGPT mini 3o           | 00:12               | 33                        |
| ‚Ü≥ mit GPT-4o mini                                                | 00:12               | 34                        |
| ‚Ü≥ mit GPT-4o                                                     | 00:14               | 34                        |
| ‚Ü≥ mit GPT-4o turbo                                               | 00:13               | 34                        |
| ‚Ü≥ mit GPT-3.5 turbo                                              | 00:13               | 30                        |
| ‚Ü≥ mit GPT-4                                                     | 00:19               | 34                        |
| medium + Purfview's Faster-Whisper-XXL + MyMemory Translate       | 00:20               | 32                        |
| large-v1 + Purfview's Faster-Whisper-XXL + Groq                   | 00:36               | 30                        |
| large-v2 + Purfview's Faster-Whisper-XXL + Winstxnhdw             | 00:17               | 29                        |
| large-v3 + Purfview's Faster-Whisper-XXL + Ollama                 | 00:33               | 33                        |
| ‚Ü≥ DeepSeek R1                                                    | 00:33               | 32                        |
| ‚Ü≥ Gemma 3                                                        | 00:33               | 33                        |
| ‚Ü≥ ZongweiGemma3                                                  | 00:33               | 31                        |
| large-turbo-v3 + Purfview's Faster-Whisper-XXL + Gemini 2.0 Flash | 00:21               | 33                        |
