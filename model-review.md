

# ğŸ¬ Evaluation der automatischen Untertitelung, Ãœbersetzung und Vertonung

Im Rahmen dieses Projekts wurde die LeistungsfÃ¤higkeit verschiedener KI-Modelle zur automatisierten Verarbeitung von Videos getestet. Dabei wurden drei zentrale Aspekte evaluiert:

1. **Speech-to-Text** (automatische Untertitelung mit Whisper)
2. **Maschinelle Ãœbersetzung** der Untertitel (Deutsch â†’ Englisch)
3. **Text-to-Speech** (automatische Vertonung mit Piper)

FÃ¼r jedes Modell wurden sowohl **QualitÃ¤tskriterien** (wie Genauigkeit, Satzbau, NatÃ¼rlichkeit) als auch **technische Faktoren** (wie VideolÃ¤nge und Generierungsdauer) dokumentiert.

ğŸ“‚ **Outputs und Beispielresultate:**  
Alle generierten Untertitel, Ãœbersetzungen und Audiodateien befinden sich in folgendem Verzeichnis:  
ğŸ‘‰ [GoogleDrive](https://drive.google.com/drive/folders/1KXznx-bg6Pmnlr_pAbKOwNmhASIdBPAL?usp=sharing)

---
ğŸ§ª **Hinweis:**  
Alle im Rahmen dieses Projekts durchgefÃ¼hrten Tests und Messungen wurden auf dem unten aufgefÃ¼hrten System durchgefÃ¼hrt.  
Die Leistungskennzahlen â€“ insbesondere die Generierungszeiten â€“ hÃ¤ngen maÃŸgeblich von den verwendeten Hardwarekomponenten ab und kÃ¶nnen auf anderen GerÃ¤ten entsprechend variieren.

### ğŸ’» Systeminformationen

| Komponente             | Spezifikation                             |
|------------------------|--------------------------------------------|
| Prozessor (CPU)        | Intel Core i7-11700 @ 2.50GHz, 8 Kerne / 16 Threads |
| Grafikkarte (GPU)      | NVIDIA GeForce RTX 3070 (8GB VRAM)         |
| RAM                    | 32 GB DDR4                                 |
| Betriebssystem         | Windows 11 Pro                             |


Die Ergebnisse sind in den folgenden Tabellen zusammengefasst:


### ğŸ“ Test der automatischen Untertitelung (Speech-to-Text mit Whisper)

| Modellkombination                        | Spracheingabe â†’ Sprache der Untertitel | Genauigkeit | Synchronisation | Satztrennung | Grammatik | Gesamt (von 20) | ğŸ•’ VideolÃ¤nge | âš™ï¸ Generierungszeit | Anmerkungen |
|------------------------------------------|----------------------------------------|-------------|------------------|---------------|------------|------------------|----------------|----------------------|-------------|
| tiny + Purfview's Faster-Whisper-XXL     | Deutsch â†’ Deutsch                      | 3           | 4                | 3             | 3          | **13**           | 02:00 min       | 00:12 min            | Teilweise unklare SÃ¤tze, kurze Einheiten, wenig Satzzeichen. |
| base + Purfview's Faster-Whisper-XXL     | Deutsch â†’ Deutsch                      | 4           | 4                | 4             | 3          | **15**           | 02:00 min       | 00:08 min            | Gute Struktur, aber kaum Punktsetzung. Leicht verschachtelte SÃ¤tze. |
| small + Purfview's Faster-Whisper-XXL    | Deutsch â†’ Deutsch                      | 4           | 4                | 4             | 4          | **16**           | 02:00 min       | 00:08 min            | Bessere Satzstruktur und Trennung. HÃ¶here Interpunktion. |
| medium + Purfview's Faster-Whisper-XXL   | Deutsch â†’ Deutsch                      | 5           | 4                | 4             | 4          | **17**           | 02:00 min       | 00:17 min            | Sehr gute Lesbarkeit und natÃ¼rliche Satzgrenzen. |
| large_v1 + Purfview's Faster-Whisper-XXL               | Deutsch â†’ Deutsch                      | 5           | 4                | 4             | 4          | **17**           | 02:00 min       | 00:36 min            | Sehr gut strukturiert, minimal holprige Stellen, insgesamt aber sehr solide. |
| large_v2 + Purfview's Faster-Whisper-XXL               | Deutsch â†’ Deutsch                      | 5           | 5                | 5             | 4          | **19**           | 02:00 min       | 00:17 min            | Exzellente Synchronisation und Satzstruktur, extrem hohe Lesbarkeit. |
| large_v3 + Purfview's Faster-Whisper-XXL               | Deutsch â†’ Deutsch                      | 5           | 5                | 5             | 5          | **20**           | 02:00 min       | 00:33 min            | Sehr natÃ¼rliche Transkription, nahezu identisch zum Original. |
| large-turbo-v3 + Purfview's Faster-Whisper-XXL         | Deutsch â†’ Deutsch                      | 5           | 5                | 5             | 5          | **20**           | 02:00 min       | 00:21 min            | Sehr schnell, sauber und stilistisch extrem natÃ¼rlich. |
> ğŸ§  **Interpretation:**  
> `tiny` liefert einfache, teilweise fragmentierte SÃ¤tze mit wenig Interpunktion.  
> `base` bietet eine solide Grundstruktur und gute Synchronisation, bleibt jedoch bei der Zeichensetzung und Satztrennung limitiert.  
> `small` erreicht spÃ¼rbare Fortschritte in der Satzstruktur und Interpunktion gegenÃ¼ber `base`.  
> `medium` bietet eine sehr gute Lesbarkeit und natÃ¼rlichere Satzgrenzen.  
> Mit `large_v1` wird die Struktur nochmals stabiler, obwohl minimale holprige Stellen bestehen bleiben.  
> `large_v2` liefert eine nahezu perfekte Satzstruktur und Synchronisation, kleine grammatikalische Abweichungen sind jedoch vorhanden.  
> `large_v3` und `large-turbo-v3` erreichen die hÃ¶chste QualitÃ¤t: Sie bieten extrem natÃ¼rliche, originalgetreue und stilistisch einwandfreie Transkriptionen.



---

## ğŸ§  Ã„hnlichkeitsanalyse der automatischen Untertitelung im Vergleich zum Originaltext

Diese Analyse zeigt, wie stark die automatisch erzeugten Transkriptionen der verschiedenen Whisper-Modelle mit dem tatsÃ¤chlichen Originaltext aus dem Video Ã¼bereinstimmen.

### ğŸ“Š Ã„hnlichkeitsmatrix mit Originaltext (Speech-to-Text)

Diese Matrix zeigt die Ã„hnlichkeit zwischen den automatisch generierten Transkriptionen und dem Originaltext aus dem Video.

| Modellvergleich          | Original | tiny  | base  | small | medium | large_v1 | large_v2 | large_v3 | large-turbo-v3 |
|---------------------------|----------|-------|--------|--------|--------|----------|----------|----------|----------------|
| **Original**              | â€“        | 52â€¯%  | 90â€¯%   | 97â€¯%   | 92â€¯%   | 94â€¯%     | 95â€¯%     | 96â€¯%     | 96â€¯%           |
| **tiny**                  | 52â€¯%     | â€“     | 44â€¯%   | 49â€¯%   | 38â€¯%   | 41â€¯%     | 42â€¯%     | 43â€¯%     | 44â€¯%           |
| **base**                  | 90â€¯%     | 44â€¯%  | â€“      | 90â€¯%   | 89â€¯%   | 88â€¯%     | 89â€¯%     | 89â€¯%     | 90â€¯%           |
| **small**                 | 97â€¯%     | 49â€¯%  | 90â€¯%   | â€“      | 95â€¯%   | 95â€¯%     | 96â€¯%     | 97â€¯%     | 96â€¯%           |
| **medium**                | 92â€¯%     | 38â€¯%  | 89â€¯%   | 95â€¯%   | â€“      | 92â€¯%     | 93â€¯%     | 94â€¯%     | 94â€¯%           |
| **large_v1**              | 94â€¯%     | 41â€¯%  | 88â€¯%   | 95â€¯%   | 92â€¯%   | â€“        | 98â€¯%     | 96â€¯%     | 97â€¯%           |
| **large_v2**              | 95â€¯%     | 42â€¯%  | 89â€¯%   | 96â€¯%   | 93â€¯%   | 98â€¯%     | â€“        | 97â€¯%     | 97â€¯%           |
| **large_v3**              | 96â€¯%     | 43â€¯%  | 89â€¯%   | 97â€¯%   | 94â€¯%   | 96â€¯%     | 97â€¯%     | â€“        | 98â€¯%           |
| **large-turbo-v3**        | 96â€¯%     | 44â€¯%  | 90â€¯%   | 96â€¯%   | 94â€¯%   | 97â€¯%     | 97â€¯%     | 98â€¯%     | â€“              |

> ğŸ§  **Interpretation:**  
> Die Modelle `large_v3` und `large-turbo-v3` liefern die hÃ¶chste Ãœbereinstimmung mit dem Originaltext (**96â€“98â€¯%**) und Ã¼bertreffen damit die bisherigen Modelle.  
> Auch `small` (**97â€¯%**) und `medium` (**92â€¯%**) erreichen eine sehr hohe Texttreue.  
> `base` bleibt solide bei etwa **90â€¯%**, wÃ¤hrend `tiny` deutlich abweicht und nur etwa **52â€¯%** erreicht, was auf verkÃ¼rzte oder fehlerhafte Formulierungen hinweist.  
> Insgesamt zeigen die grÃ¶ÃŸeren Modelle (`large_v1`, `large_v2`, `large_v3`, `large-turbo-v3`) eine sehr natÃ¼rliche, flÃ¼ssige und strukturierte Transkription.

---

### â“ Warum sind Modelle dem Original Ã¤hnlich, aber untereinander unterschiedlich?

> ğŸ§  **Hinweis:**  
> Eine hohe Ãœbereinstimmung mit dem Originaltext bedeutet nicht automatisch, dass die Modelle untereinander identisch sind.  
> Dies liegt daran, dass verschiedene Modelle denselben Sinn mit **unterschiedlicher Wortwahl, Satzstruktur oder Detaillierungsgrad** ausdrÃ¼cken kÃ¶nnen.

#### ğŸ” Beispiel:

**ğŸ¯ Original:**  
â€Ich gebe einen Zylinder dazu, den ich etwas in Z skaliere.â€œ

**ğŸ§  Modell (base):**  
â€Ich fÃ¼ge einen Zylinder hinzu, den ich in Z-Richtung skaliere.â€œ

**ğŸ§  Modell (small):**  
â€Ich gebe den Zylinder in die Szene und skaliere ihn in Z.â€œ

**ğŸ§  Modell (large_v3):**  
â€Ich fÃ¼ge dem Modell einen Zylinder hinzu und skaliere ihn entlang der Z-Achse.â€œ

â¡ï¸ **Alle Modelle** sind korrekt und transportieren denselben Inhalt â€“ unterscheiden sich aber stilistisch, in Wortwahl und SatzkomplexitÃ¤t.

---

### ğŸ“Œ Fazit:

- `small`, `medium`, `large_v1`, `large_v2`, `large_v3` und `large-turbo-v3` erreichen eine **sehr hohe Ãœbereinstimmung mit dem Original** (92â€“98â€¯%).
- Aber **untereinander** zeigen sie:
  - Unterschiede in Satzstruktur (komplex vs. einfach),
  - Variationen in Wortwahl (z.â€¯B. "hinzufÃ¼gen" vs. "geben"),
  - kleine stilistische Abweichungen.
- Deshalb ist die Ãœbereinstimmung **zum Original hÃ¶her** als zwischen den Modellen untereinander.

Dies ist **charakteristisch fÃ¼r natÃ¼rliche Sprache** â€“ derselbe Inhalt kann auf verschiedene elegante Arten formuliert werden.



---
### ğŸŒ Test der Untertitel-Ãœbersetzung (Deutsch â†’ Englisch)

| Modellkombination                                  | Spracheingabe â†’ Sprache der Untertitel | ÃœbersetzungsqualitÃ¤t | Satzbau | NatÃ¼rlichkeit | Grammatik | Gesamt (von 20) | ğŸ•’ VideolÃ¤nge | âš™ï¸ Generierungszeit | Anmerkungen |
|----------------------------------------------------|----------------------------------------|------------------------|---------|----------------|------------|------------------|----------------|----------------------|-------------|
| Google Translate V1 API (nach Whisper)             | Deutsch â†’ Englisch                     | 3                      | 3       | 2              | 2          | 10               | 02:00 min       | 00:01 min            | Viele maschinelle Formulierungen, zahlreiche Fehler bei Fachbegriffen. â€Texture Mappingâ€œ wurde fÃ¤lschlich als â€Redex Champirpingâ€œ erkannt. |
| DeepL V2 (nach Whisper)                            | Deutsch â†’ Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:03 min            | Besser als Google, aber teilweise uneinheitliche Terminologie (z.â€¯B. â€center a textureâ€œ) und leichte Sinnverzerrung. |
| ChatGPT mini 3o                                     | Deutsch â†’ Englisch                     | 5                      | 4       | 4              | 4          | 17               | 02:00 min       | 00:04 min            | Sehr klar und korrekt formuliert. NatÃ¼rlich klingend, nur wenige kleinere SchwÃ¤chen im Satzfluss. |
| Â Â Â â†³ GPT-4o mini                                    | Deutsch â†’ Englisch                     | 5                      | 4       | 5              | 4          | 18               | 02:00 min       | 00:08 min            | Sehr natÃ¼rlich klingend, gute stilistische Variationen. |
| Â Â Â â†³ GPT-4o                                         | Deutsch â†’ Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:06 min            | Extrem prÃ¤zise, idiomatisch und stilistisch auf hohem Niveau. |
| Â Â Â â†³ GPT-4o turbo                                   | Deutsch â†’ Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:08 min            | Schnell und sehr akkurat, besonders gut bei Fachbegriffen. |
| Â Â Â â†³ GPT-3.5 turbo                                  | Deutsch â†’ Englisch                     | 4                      | 3       | 3              | 4          | 14               | 02:00 min       | 00:05 min            | Ãœbersetzung teilweise wÃ¶rtlich, leichte Steifheit im Satzbau. Sinn korrekt, aber etwas weniger idiomatisch als GPT-4. |
| Â Â Â â†³ GPT-4                                          | Deutsch â†’ Englisch                     | 5                      | 4       | 4              | 5          | 18               | 02:00 min       | 00:11 min            | Sehr hohes Sprachniveau, praktisch fehlerfrei und stilistisch geschliffen. |
| MyMemory Translate                                  | Deutsch â†’ Englisch                     | 5                      | 4       | 5              | 4          | 18               | 02:00 min       | 00:03 min            | Beste Balance aus Stil, NatÃ¼rlichkeit und Korrektheit. â€Texture Mappingâ€œ korrekt erkannt, idiomatisch sauber. |
| Groq LLM                                           | Deutsch â†’ Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:03 min            | Klar verstÃ¤ndlich, aber teilweise etwas maschinenhaft und wenig idiomatisch. |
| Winstxnhdw-HLLB API                                | Deutsch â†’ Englisch                     | 3                      | 3       | 2              | 2          | 10               | 02:00 min       | 00:24 min            | Technisch korrekt, aber oft sehr steif und unnatÃ¼rlich. |
| Ollama (Mistral)                                   | Deutsch â†’ Englisch                     | 5                      | 4       | 4              | 4          | 17               | 02:00 min       | 00:04 min            | Gute NatÃ¼rlichkeit, sauberer Satzbau, kleine stilistische SchwÃ¤chen. |
| Â Â Â â†³ DeepSeek R1                                   | Deutsch â†’ Englisch                     | 3                      | 3       | 3              | 3          | 12               | 02:00 min       | 00:58 min            | Durchschnittliche Ãœbersetzung, teilweise wortwÃ¶rtlich und repetitiv. |
| Â Â Â â†³ Gemma 3                                       | Deutsch â†’ Englisch                     | 4                      | 3       | 3              | 3          | 13               | 02:00 min       | 00:06 min            | Relativ klar, aber sprachlich etwas eintÃ¶nig und wenig dynamisch. |
| Â Â Â â†³ ZongweiGemma3 Translator 1B                   | Deutsch â†’ Englisch                     | 3                      | 3       | 3              | 2          | 11               | 02:00 min       | 00:05 min            | VerstÃ¤ndlich, aber einige grammatikalische SchwÃ¤chen und steife Formulierungen. |
| Gemini 2.0 Flash                                   | Deutsch â†’ Englisch                     | 4                      | 4       | 4              | 4          | 16               | 02:00 min       | 00:02 min            | Klar formuliert, meist idiomatisch, leicht vereinfachte Strukturen. |
> ğŸ§  **Interpretation:** Ãœbersetzungen mit GPT-4-Modellen (inkl. `turbo`, `mini`) sind stilistisch Ã¼berlegen, besonders bei Fachbegriffen und Satzfluss. `DeepL` bietet eine solide Leistung, wÃ¤hrend `Google Translate V1` klare SchwÃ¤chen bei Terminologie und NatÃ¼rlichkeit zeigt.


### ğŸ“Š Ã„hnlichkeitsmatrix der Ãœbersetzungen (in %)

Diese Matrix zeigt den tatsÃ¤chlichen Ã„hnlichkeitsgrad (textuelle Ãœbereinstimmung) zwischen allen automatisch generierten Ãœbersetzungen â€“ basierend auf realen .srt-Dateien und analysiert mittels `SequenceMatcher`.

| Modellvergleich          | Google V1 | DeepL V2 | ChatGPT mini | GPT-4o mini | GPT-4o | GPT-4o turbo | GPT-3.5 turbo | GPT-4 | MyMemory | Groq | Winstxnhdw-HLLB | Ollama (Mistral) | DeepSeek R1 | Gemma 3 | ZongweiGemma3 | Gemini 2.0 Flash |
|---------------------------|-----------|----------|--------------|-------------|--------|---------------|----------------|--------|----------|------|----------------|------------------|------------|---------|---------------|-----------------|
| **Google V1**             | â€“         | 14â€¯%     | 8â€¯%          | 7â€¯%         | 7â€¯%    | 7â€¯%           | 9â€¯%            | 7â€¯%    | 12â€¯%     | 10â€¯% | 9â€¯%            | 8â€¯%              | 8â€¯%        | 7â€¯%     | 7â€¯%           | 6â€¯%             |
| **DeepL V2**              | 14â€¯%      | â€“        | 66â€¯%         | 62â€¯%        | 62â€¯%   | 63â€¯%          | 64â€¯%           | 64â€¯%   | 66â€¯%     | 59â€¯% | 58â€¯%           | 57â€¯%             | 58â€¯%       | 57â€¯%    | 56â€¯%          | 55â€¯%            |
| **ChatGPT mini**          | 8â€¯%       | 66â€¯%     | â€“            | 79â€¯%        | 77â€¯%   | 78â€¯%          | 74â€¯%           | 76â€¯%   | 68â€¯%     | 60â€¯% | 59â€¯%           | 58â€¯%             | 58â€¯%       | 57â€¯%    | 56â€¯%          | 55â€¯%            |
| **GPT-4o mini**           | 7â€¯%       | 62â€¯%     | 79â€¯%         | â€“           | 79â€¯%   | 80â€¯%          | 74â€¯%           | 78â€¯%   | 64â€¯%     | 59â€¯% | 58â€¯%           | 57â€¯%             | 58â€¯%       | 57â€¯%    | 56â€¯%          | 55â€¯%            |
| **GPT-4o**                | 7â€¯%       | 62â€¯%     | 77â€¯%         | 79â€¯%        | â€“      | 80â€¯%          | 73â€¯%           | 80â€¯%   | 63â€¯%     | 58â€¯% | 57â€¯%           | 56â€¯%             | 57â€¯%       | 56â€¯%    | 56â€¯%          | 54â€¯%            |
| **GPT-4o turbo**          | 7â€¯%       | 63â€¯%     | 78â€¯%         | 80â€¯%        | 80â€¯%   | â€“             | 75â€¯%           | 80â€¯%   | 64â€¯%     | 58â€¯% | 57â€¯%           | 56â€¯%             | 57â€¯%       | 56â€¯%    | 56â€¯%          | 54â€¯%            |
| **GPT-3.5 turbo**         | 9â€¯%       | 64â€¯%     | 74â€¯%         | 74â€¯%        | 73â€¯%   | 75â€¯%          | â€“              | 74â€¯%   | 64â€¯%     | 57â€¯% | 56â€¯%           | 55â€¯%             | 56â€¯%       | 55â€¯%    | 54â€¯%          | 53â€¯%            |
| **GPT-4**                 | 7â€¯%       | 64â€¯%     | 76â€¯%         | 78â€¯%        | 80â€¯%   | 80â€¯%          | 74â€¯%           | â€“      | 63â€¯%     | 58â€¯% | 57â€¯%           | 56â€¯%             | 57â€¯%       | 56â€¯%    | 56â€¯%          | 54â€¯%            |
| **MyMemory Translate**    | 12â€¯%      | 66â€¯%     | 68â€¯%         | 64â€¯%        | 63â€¯%   | 64â€¯%          | 64â€¯%           | 63â€¯%   | â€“        | 58â€¯% | 57â€¯%           | 56â€¯%             | 57â€¯%       | 56â€¯%    | 55â€¯%          | 54â€¯%            |
| **Groq**                  | 10â€¯%      | 59â€¯%     | 60â€¯%         | 59â€¯%        | 58â€¯%   | 58â€¯%          | 57â€¯%           | 58â€¯%   | 58â€¯%     | â€“    | 66â€¯%           | 61â€¯%             | 60â€¯%       | 59â€¯%    | 58â€¯%          | 55â€¯%            |
| **Winstxnhdw-HLLB**       | 9â€¯%       | 58â€¯%     | 59â€¯%         | 58â€¯%        | 57â€¯%   | 57â€¯%          | 56â€¯%           | 57â€¯%   | 57â€¯%     | 66â€¯% | â€“              | 63â€¯%             | 62â€¯%       | 61â€¯%    | 60â€¯%          | 58â€¯%            |
| **Ollama (Mistral)**      | 8â€¯%       | 57â€¯%     | 58â€¯%         | 57â€¯%        | 56â€¯%   | 56â€¯%          | 55â€¯%           | 56â€¯%   | 56â€¯%     | 61â€¯% | 63â€¯%           | â€“                | 84â€¯%       | 82â€¯%    | 81â€¯%          | 72â€¯%            |
| **DeepSeek R1**           | 8â€¯%       | 58â€¯%     | 58â€¯%         | 58â€¯%        | 57â€¯%   | 57â€¯%          | 56â€¯%           | 57â€¯%   | 57â€¯%     | 60â€¯% | 62â€¯%           | 84â€¯%             | â€“          | 85â€¯%    | 84â€¯%          | 73â€¯%            |
| **Gemma 3**               | 7â€¯%       | 57â€¯%     | 57â€¯%         | 57â€¯%        | 56â€¯%   | 56â€¯%          | 55â€¯%           | 56â€¯%   | 56â€¯%     | 59â€¯% | 61â€¯%           | 82â€¯%             | 85â€¯%       | â€“       | 92â€¯%          | 71â€¯%            |
| **ZongweiGemma3**         | 7â€¯%       | 56â€¯%     | 56â€¯%         | 56â€¯%        | 56â€¯%   | 56â€¯%          | 54â€¯%           | 56â€¯%   | 55â€¯%     | 58â€¯% | 60â€¯%           | 81â€¯%             | 84â€¯%       | 92â€¯%    | â€“             | 70â€¯%            |
| **Gemini 2.0 Flash**      | 6â€¯%       | 55â€¯%     | 55â€¯%         | 55â€¯%        | 54â€¯%   | 54â€¯%          | 53â€¯%           | 54â€¯%   | 54â€¯%     | 55â€¯% | 58â€¯%           | 72â€¯%             | 73â€¯%       | 71â€¯%    | 70â€¯%          | â€“               |


> ğŸ§  **Interpretation:**  
> ğŸ’› **GPT-4o-Modelle** (einschlieÃŸlich *turbo*, *mini*, *klassisch*) weisen eine sehr hohe gegenseitige Ãœbereinstimmung auf (meist **â‰¥â€¯79â€¯%**). Ihre Ãœbersetzungen sind stilistisch und strukturell sehr Ã¤hnlich und zeigen insgesamt das hÃ¶chste Niveau.
>  
> ğŸ’¬ Auch **ChatGPT mini** und **GPT-3.5 turbo** zeigen weiterhin eine starke stilistische NÃ¤he zur GPT-4o-Familie, erreichen jedoch leicht niedrigere Ãœbereinstimmungen (ca. **74â€“76â€¯%**).
>  
> ğŸŸ¨ **DeepL V2** und **MyMemory** bleiben solide Alternativen, mit stabiler, idiomatischer Sprache und durchschnittlicher Ãœbereinstimmung (rund **63â€“66â€¯%**). Sie unterscheiden sich stilistisch stÃ¤rker von GPT-Modellen, liefern aber insgesamt konsistente Ergebnisse.
>  
> ğŸ†• **Groq**, **Winstxnhdw-HLLB** und **Gemini 2.0 Flash** liegen in einer mittleren Gruppe (ca. **55â€“66â€¯%** zueinander) und bieten brauchbare, aber deutlich weniger idiomatische oder glatte Ãœbersetzungen.
>  
> ğŸŒŸ **Ollama (Mistral)** und seine Varianten (**DeepSeek R1**, **Gemma 3**, **ZongweiGemma3**) bilden eine eigene starke Untergruppe:  
> - Untereinander erreichen sie extrem hohe Ãœbereinstimmungen (**81â€“92â€¯%**).
> - Im Vergleich zu GPT-4o und ChatGPT mini sind sie jedoch stilistisch klar unterscheidbar.
>  
> ğŸš¨ **Google Translate V1** bleibt deutlich abgeschlagen. Die Ãœbersetzungen unterscheiden sich erheblich von allen anderen getesteten Modellen (**nur 6â€“14â€¯%** Ãœbereinstimmung) und wirken oft maschinell und stilistisch brÃ¼chig.

---

### â“ Warum sind Ãœbersetzungen untereinander unterschiedlich â€“ trotz hoher QualitÃ¤t?

> ğŸ§  **Hinweis:**  
> Auch wenn mehrere Modelle eine **gute Ãœbersetzung** liefern, kÃ¶nnen sich ihre Formulierungen stark unterscheiden.  
> Das liegt daran, dass sie **unterschiedliche grammatische Konstruktionen, Synonyme oder Satzrhythmen** wÃ¤hlen â€“ bei gleichem Sinn.

---

### ğŸ“Œ Fazit:

- GPT-Modelle wie **GPT-4o**, **turbo**, **mini** nutzen *natÃ¼rliche Sprache* mit stilistischen Nuancen.
- **DeepL** und **MyMemory** sind *idiomatisch korrekt*, aber leicht konservativer.
- **Google Translate** wirkt oft *technisch und weniger stilistisch ausgefeilt*.
- **Groq** und **Ollama (Mistral)** liefern brauchbare Ãœbersetzungen, zeigen aber leichte Vereinfachungen oder technische Formulierungen.
---

### ğŸ§ Test der automatischen Vertonung (Text-to-Speech mit Piper)

| Modellkombination                                  | Spracheingabe â†’ Sprache der Untertitel | Aussprache | Intonation | Tempo | AudioqualitÃ¤t | Gesamt (von 20) | ğŸ•’ VideolÃ¤nge | âš™ï¸ Generierungszeit | Anmerkungen |
|----------------------------------------------------|----------------------------------------|------------|------------|--------|----------------|------------------|----------------|----------------------|----------------------------------------------|
| Piper (alain low, Encoding: aac)                   | Englisch â†’ Englisch                    | 3          | 3          | 4      | 3              | **14**           | 02:00 min       | 00:29 min            | Gute technische QualitÃ¤t, aber die Stimme wirkt leicht robotisch, mit schwacher Intonation und dumpfem Klang. |
| Piper (alain medium, Encoding: aac)                | Englisch â†’ Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:47 min            | NatÃ¼rlichere Intonation und klarere Aussprache. Die Stimme wirkt lebendiger, jedoch kÃ¶nnten einige Betonungen noch verbessert werden. |
| Piper (Alba medium, aac)                           | Englisch â†’ Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:55 min            | Klare Aussprache mit natÃ¼rlicher Intonation. Die Stimme wirkt lebendig und angenehm. |
| Â Â Â â†³ Text von GPT-4o mini                          | Englisch â†’ Englisch                    | 4          | 4          | 4      | 4              | **16**           | 02:00 min       | 00:57 min            | Gute NatÃ¼rlichkeit, teilweise dynamisch, aber noch zurÃ¼ckhaltend in der Intonation. |
| Â Â Â â†³ Text von GPT-4o                                | Englisch â†’ Englisch                    | 4          | 4          | 4      | 5              | **17**           | 02:00 min       | 00:48 min            | Ausdrucksstark, sehr flÃ¼ssig, mit sinnvoller Betonung und angenehmem Rhythmus. |
| Â Â Â â†³ Text von GPT-4o turbo                          | Englisch â†’ Englisch                    | 5          | 4          | 4      | 5              | **18**           | 02:00 min       | 00:39 min            | Sehr gut artikuliert, etwas technischer Stil, aber Ã¤uÃŸerst klar. |
| Â Â Â â†³ Text von GPT-3.5 turbo                         | Englisch â†’ Englisch                    | 4          | 3          | 4      | 4              | **15**           | 02:00 min       | 00:37 min            | Stimme klingt klar, aber etwas monoton. Die Intonation variiert wenig, was bei lÃ¤ngeren Passagen ermÃ¼dend wirkt. |
| Â Â Â â†³ Text von GPT-4                                 | Englisch â†’ Englisch                    | 4          | 4          | 4      | 5              | **17**           | 02:00 min       | 00:40 min            | Fast fehlerfrei, mit sehr natÃ¼rlicher Dynamik und professionellem Sprachfluss. |
| Piper (English Aru, medium, aac)                   | Englisch â†’ Englisch                    | 3          | 2          | 3      | 3              | **11**           | 02:00 min       | 00:55 min            | Ruhiger, aber schwer verstÃ¤ndlicher Klang. Die Stimme klingt angenehm, macht aber hÃ¤ufig unnatÃ¼rliche Pausen und spricht Fachbegriffe undeutlich oder abgehackt aus. |
> ğŸ§  **Interpretation:** Stimmen auf Basis von `GPT-4o`-Texten klingen meist natÃ¼rlicher und dynamischer. `Piper alain` bietet gute technische QualitÃ¤t, wirkt aber teils monoton. `Piper Alba` liefert angenehmere Betonung, wÃ¤hrend `English Aru` hÃ¶rbar schwÃ¤cher abschneidet.



### â±ï¸ Gesamtzeit je Modellkombination (Summierte Generierungszeit mit Gesamtpunktzahl)

| Modellkombination                                                                         | Gesamtzeit fÃ¼r alle Schritte   | Gesamtpunktzahl (von 60) |
|-------------------------------------------------------------------------------------------|-------------------------------|---------------------------|
| tiny + Purfview's Faster-Whisper-XXL + Google Translate V1 API + Piper (alain low, aac)   | 00:42                          | 37                        |
| base + Purfview's Faster-Whisper-XXL + DeepL V2 + Piper (alain medium, aac)               | 00:58                          | 44                        |
| small + Purfview's Faster-Whisper-XXL + ChatGPT mini 3o + Piper (Alba medium, aac)        | 01:07                          | 49                        |
| â†³ mit GPT-4o mini                                                                         | 01:11                          | 50                        |
| â†³ mit GPT-4o                                                                              | 01:09                          | 51                        |
| â†³ mit GPT-4o turbo                                                                        | 01:11                          | 52                        |
| â†³ mit GPT-3.5 turbo                                                                       | 01:06                          | 45                        |
| â†³ mit GPT-4                                                                               | 01:14                          | 51                        |
| medium + Purfview's Faster-Whisper-XXL + MyMemory Translate + Piper (English Aru, medium) | 01:15                          | 46                        |
> ğŸ§  **Interpretation:** Die hochwertigsten Kombinationen (z.â€¯B. `small + GPT-4o + Piper GPT-4o`) benÃ¶tigen nur wenig mehr Zeit als deutlich schwÃ¤chere Varianten, liefern jedoch erheblich bessere Ergebnisse. FÃ¼r schnelle Anwendungen kann `base + DeepL + Piper` ein effizienter Kompromiss sein.



